%! TEX root = ./main.tex
\documentclass{beamer}

\usepackage{mpemath}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

%Information to be included in the title page:
\title{ROIL -- Robust Offline Imitation Learning}
\author{Gersi Doko}
\institute{Dept. of Computer Science, University of New Hampshire}
\date{2024}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Introduction}
	\begin{itemize}
		\item IL is a learning paradigm where an agent learns a policy from expert demonstrations.
		\item We model the domain as a Markov Decision Process (MDP) ($\mathcal{S}, \mathcal{A}, \mathcal{P}, p_0, r, \gamma$).
		\item We aim to perform well, even in the presence of covariate shift -- where the experts state visitation distribution does not follow their own --.
	\end{itemize}
\end{frame}

\begin{frame}
	% TODO: AVOID SAYING WE
	\frametitle{Preliminaries}
	We are given a dataset of state, action pairs $D_e$ generated by some expert policy $\pi_e$.
	\[ D_e = (s_i, \pi_e(s_i))_{i=1}^N \]
	We aim to learn a policy $\pi$ that performs well in the MDP, without access to the true reward function $r^\star$,
	that $\pi_e$ follows.

	\[ \mathcal{W} = \lbrace w \in \Real^k \mid \lvert \lvert w \rvert \rvert_1 \leq 1 \rbrace \]
	We assume that $\exists w \in \mathcal{W} \mid r^\star = \Phi w$.

\end{frame}

\begin{frame}
	\frametitle{Preliminaries}
	\[ \rho(\pi, r) = \lim_{t \to \infty} \mathbb{E}^{\pi, \mathcal{P}} \lbrack \gamma^t r(s_t, \pi(s_t)) \rbrack \]
	\[ \min_{\pi \in \Pi} \max_{r \in \mathcal{R}} \rho(\pi_e, r) - \rho(\pi, r)\]
\end{frame}

\end{document} 
