\documentclass{beamer}

\usepackage{mpemath}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{pgf}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{natbib}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{bbm}


\usetikzlibrary{arrows,automata,backgrounds,positioning,decorations,intersections,matrix}

% *** Styles ***
\usetheme{Singapore}
\setbeamertemplate{navigation symbols}{}
% \usetheme[progressbar=frametitle]{metropolis}
% \usecolortheme{dolphin}
%\useinnertheme{circles}
%\usecolortheme{rose}
%\setbeamercovered{transparent}
\setbeamercovered{invisible}
\usefonttheme{professionalfonts}
%\usefonttheme[onlymath]{serif}
\setbeamertemplate{footline}[frame number]

\title{ROIL -- Robust Offline Imitation Learning}
\author{Gersi Doko}
\institute{Department of Computer Science \\ University of New Hampshire}
% \date{2024}

\AtBeginSection[]{
	\begin{frame}
          \vfill
	\centering
	% \usebeamerfont{title}
        {\huge\bf \insertsectionhead}%
	\vfill
\end{frame}
}

\begin{document}

\frame{\titlepage}

\section*{Intro}

\begin{frame}
	\frametitle{Robust Offline Imitation Learning}
	\textbf{Objective}: Learn a policy from expert demonstrations
	\begin{itemize}
	\item Health care: automating and improving ER care
	\item Robotics: self-driving cars, complete tasks only from demonstrations
	\item Retail: recommendation systems, customer service
	
	\end{itemize}
	\vfill
	\textbf{Offline IL}: Compute policy from a fixed dataset of expert demonstrations
	\begin{itemize}
		\item No interaction with the environment
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{This Talk}
	Computing policies that minimize worst case regret, with respect to a possible set of experts.
	\vfill
	\textbf{Outline}
	\begin{itemize}
	\item \emph{Motivation}: Why study IRL?
	\item \emph{Domain Introduction}: What is IRL?
	\item \emph{Robust Offline IL}: New approach to minimizing regret of learning from demonstrations
	\end{itemize}
\end{frame}

\section*{Motivation}

\begin{frame}
\frametitle{Motivation}
	\begin{itemize}
		\item Solve problems where it's hard to define what outcome we want.
		\item Self-driving, Robotic Control, Medicine, Recommendation Systems, etc.
		\item Often have access to expert demonstrations instead of a clear description for the task.
	\end{itemize}
\end{frame}

\section*{Domain Introduction}

\begin{frame}
\frametitle{Domain Introduction}
	\begin{itemize}
		\item IRL is a learning paradigm where an agent learns a policy from expert demonstrations.
		\item Similar to Reinforcement Learning, but we don't have access to the reward function.
		\item Instead, we try and replicate the expert's behavior given their demonstrations.
	\end{itemize}
\end{frame}

\begin{frame} \frametitle{Markov Decision Process}
  \textbf{Model} (tabular in this talk) \par
    {\small
   ~~~States $\mathcal{S}$: $s_1, s_2, s_3, \dots $ \par
   ~~~Actions $\mathcal{A}$: $a_1, a_2, \dots $ \par
   ~~~Transition probabilities $\mathcal{P}$ \par
   ~~~Initial state distribution $p_0$ \par
   ~~~Discount factor $\gamma$ \par
   ~~~\textbf{Rewards $r$}}
    \vfill 
    \textbf{Solution}: Policy ${\pi}\colon \mathcal{S} \to \Delta^\mathcal{A}$
    \vfill
    \textbf{Return}: Discounted random return (random over trajectories):
    \[
      \tilde{\rho}(\pi) = \sum_{t=0}^\infty \gamma^t r(\tilde{s}^{\pi}_t, \tilde{a}^{{\pi}}_t)
    \]
    \vfill
    \textbf{Random variables}: $\tilde{\rho}, \tilde{s}, \tilde{a}, \tilde{x}, \dots $ adorned with tilde
\end{frame}

\begin{frame}
	\frametitle{Domain Introduction}
	\begin{itemize}
		\item We model the environment as a Markov Decision Process (MDP) $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, p_0, r, \gamma)$. Where:
		\item $\mathcal{S}$ is the state space.
		\item $\mathcal{A}$ is the action space.
		\item $\mathcal{P}$ is the probability transition tensor $\mathcal{P}(s_t, a_t) \in \Delta^\mathcal{S}$.
		\item $p_0$ is the initial state distribution.
		\item $r$ is the reward function.
		\item $\gamma$ is the discount factor.
	\end{itemize}
\end{frame}

\begin{frame}
	% TODO: AVOID SAYING WE
	\frametitle{Domain Introduction}

	We are given a dataset of state, action pairs $D_e$ generated by some expert policy $\pi_e$.
	\[ D_e = (s_i, \pi_e(s_i))_{i=1}^N \]
	We aim to learn a policy $\pi$ that performs well in the MDP, without access to the true reward function $r^\star$,
	that $\pi_e$ follows.

	\[ \mathcal{W} = \lbrace w \in \Real^k \mid \lvert \lvert w \rvert \rvert_1 \leq 1 \rbrace \]
	We assume that $\exists w \in \mathcal{W} \mid r^\star = \Phi w$.

\end{frame}

\section*{Problem and Solution}

\begin{frame}
	\frametitle{Problem and Solution}
	\[ \rho(\pi, r) = \lim_{t \to \infty} \mathbb{E}^{\pi, \mathcal{P}} \lbrack \gamma^t r(s_t, \pi(s_t)) \rbrack \]
	\[ \min_{\pi \in \Pi} \max_{r \in \mathcal{R}} \rho(\pi_e, r) - \rho(\pi, r)\]
\end{frame}

\end{document} 
