\documentclass{beamer}

\usepackage{mpemath}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\title{ROIL -- Robust Offline Imitation Learning}
\author{Gersi Doko}
\institute{Department of Computer Science, University of New Hampshire}
\date{2024}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Motivation}
	\begin{itemize}
		\item Solve problems where it's hard to define what outcome we want.
		\item Self-driving, Robotic Control, Medicine, Recommendation Systems, etc.
		\item Often have access to expert demonstrations instead of a clear description for the task.
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Domain Introduction}
	\begin{itemize}
		\item IRL is a learning paradigm where an agent learns a policy from expert demonstrations.
		\item Similar to Reinforcement Learning, but we don't have access to the reward function.
		\item Instead, we try and replicate the expert's behavior given their demonstrations.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Domain Introduction}
	\begin{itemize}
		\item We model the environment as a Markov Decision Process (MDP) $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, p_0, r, \gamma)$. Where:
		\item $\mathcal{S}$ is the state space.
		\item $\mathcal{A}$ is the action space.
		\item $\mathcal{P}$ is the probability transition tensor $\mathcal{P}(s_t, a_t) \in \Delta^\mathcal{S}$.
		\item $p_0$ is the initial state distribution.
		\item $r$ is the reward function.
		\item $\gamma$ is the discount factor.
	\end{itemize}
\end{frame}

\begin{frame}
	% TODO: AVOID SAYING WE
	\frametitle{Domain Introduction}

	We are given a dataset of state, action pairs $D_e$ generated by some expert policy $\pi_e$.
	\[ D_e = (s_i, \pi_e(s_i))_{i=1}^N \]
	We aim to learn a policy $\pi$ that performs well in the MDP, without access to the true reward function $r^\star$,
	that $\pi_e$ follows.

	\[ \mathcal{W} = \lbrace w \in \Real^k \mid \lvert \lvert w \rvert \rvert_1 \leq 1 \rbrace \]
	We assume that $\exists w \in \mathcal{W} \mid r^\star = \Phi w$.

\end{frame}

\begin{frame}
	\frametitle{Problem and Solution}
	\[ \rho(\pi, r) = \lim_{t \to \infty} \mathbb{E}^{\pi, \mathcal{P}} \lbrack \gamma^t r(s_t, \pi(s_t)) \rbrack \]
	\[ \min_{\pi \in \Pi} \max_{r \in \mathcal{R}} \rho(\pi_e, r) - \rho(\pi, r)\]
\end{frame}

\end{document} 
