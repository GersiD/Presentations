\begin{block}{Prior Work: Weight-Select-Update (WSU)}
% \begin{itemize}
%     \item WSU: constant prior distribution of model weights, $\lambda$
%     \item CADP: adjustable posterior distribution of model weights, $b$
% \end{itemize}

 \vspace{0.3cm}
\begin{center}
\fbox{
\begin{minipage}{.97\textwidth}
\begin{footnotesize}
\textbf{WSU Approximation Algorithm}
\end{footnotesize}

\vspace{0.2cm}
  \textbf{Input:} MMDPs,  Model weights \alert{$\lambda$ }\\
  \textbf{Output:} $\pi = (\pi_1,\ldots,\pi_T)$
    \begin{enumerate}
        \item Initialize $v_{T+1,m}^{\pi}(s_{T+1}) = 0, 
         \forall m \in \mathcal{M} $ 
         \item \text{For}{$\quad t = T, T-1, \dots , 1 \quad$}\text{do}
         %\item $\quad $ \text{For}{ Every state $s_t \in \mathcal{S}$}$\quad $\text{do}
         \item $\quad \quad$ $\pi_t(s_t) \in \arg\max_{a \in \mathcal{A}} \sum_{m \in \mathcal{M}} \alert{\lambda_m} \cdot q_{t,m}^{\pi}(s_t, a),  \quad \forall s_t\in \mathcal{S}.$
         \vspace{0.2cm}
         \item $\quad \quad$ $  v_{t,m}^\pi (s_t) = r_t^{m}(s_t,\pi(s_t))  + \sum_{s_{t+1} \in \mathcal{S}} p_t^m(s_{t+1} \mid  s_t,\pi(s_t)) \cdot v_{t+1,m}^{\pi}(s_t+1), \forall  m \in \mathcal{M}$ . 
        \item \text{end for}
    \end{enumerate}
\end{minipage}
}
\end{center}
\end{block}

\begin{block}{MMDP Policy Gradient}

  \begin{itemize}
  \item \alert{Main idea}: Take a coordinate ascent perspective to adjust model weights iteratively.
  % \vspace{0.3cm}
       \item \textbf{Definition 4.1} An \emph{adjustable weight} for each $m\in \mathcal{M}$, $\pi\in \Pi$, $t\in \mathcal{T}$, and $s \in \mathcal{S}$ is
      \[
       b_{t,m}^{\pi}(s) \; =\;  \mathop{\mathbb{P}}[\tilde{m} = m, \tilde{s}_t= s],
       \]
     where $S_0 \sim \mu$, $\tilde{m} \sim \lambda$, and $\tilde{s}_1, \dots , \tilde{s}_T$ are distributed according to $p^{\tilde{m}}$ of policy $\pi$.
    \vspace{0.2cm}
      \item \textbf{Theorem 4.1}: Gradient of $\rho$ in Eq.~(1) for each $t \in \mathcal{T}$, $\hat{s} \in \mathcal{S}$, $\hat{a} \in \mathcal{A}$, and $\pi \in \Pi_R $ is
     \[
     \frac{\partial \rho(\pi)}{\partial\pi_t(\hat{s},\hat{a})}\; =\;
      \sum_{m \in \mathcal{M}} b_{t,m}^{\pi}(\hat{s})\cdot q_{t,m}^{\pi}(\hat{s},\hat{a})\,,
     \]
     where $q$ is state-action value function and $b$ is an adjustable weight
     \vspace{0.2cm}
     \item \textbf{Corollary 4.2} For any $\bar{\pi} \in \Pi$ and $t\in \mathcal{T}$, function $\pi_t \mapsto \rho(\bar{\pi}_1, \dots , \pi_t, \dots , \bar{\pi}_T)$ is \emph{linear}.  
     \vspace{0.2cm}

    \item Linearity implies that we can solve the maximization over $\pi_t(s)$ as
    \begin{equation*}
 \pi_t^n(s)  \in \argmax_{a \in \mathcal{A}} \sum_{m \in \mathcal{M}} \alert{b_{t,m}^{\pi^{n-1}}}(s)  \cdot q_{t,m}^{\pi^n} (s,a) .
\end{equation*}
  
   \end{itemize}
\end{block}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "poster"
%%% End: 
